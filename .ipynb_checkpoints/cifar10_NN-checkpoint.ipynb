{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kozyrevsky-V\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_lable(y, num_classes):\n",
    "    y_gt = np.zeros((len(y), num_classes))\n",
    "    for i in range(0, len(y)):\n",
    "        y_gt[i, y[i]] = 1\n",
    "    return y_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta, X):\n",
    "    return 1.0 / (1 + np.exp(-np.dot(X, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm_derivative(theta, X):\n",
    "    return h(theta, X) * (1 - h(theta, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_gt, y_pred):\n",
    "    cross_entropy_error = 0.0\n",
    "    for i in range(0, len(y_gt)):\n",
    "        for j in range(0, len(y_gt[i])):\n",
    "            cross_entropy_error -= (y_gt[i][j] * np.log(y_pred[i][j]) + (1 - y_gt[i][j]) * np.log(1 - y_pred[i][j]))\n",
    "    return cross_entropy_error / len(y_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEderivative(X, y_gt, y_pred):\n",
    "    return np.dot(y_pred - y_gt, X) / len(y_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_test, y_out):\n",
    "    accuracy = 0\n",
    "    for i in range(len(y_out)):\n",
    "        if np.argmax(y_test[i]) == np.argmax(y_out[i]):\n",
    "            accuracy += 1\n",
    "    return accuracy / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSprob(Et_1, g, gamma, mu, e):\n",
    "    Et = (gamma*Et_1) + (1-gamma)*(g**2)\n",
    "    delta = (mu)/np.sqrt(Et + e)\n",
    "    delta*=g\n",
    "    return Et, delta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adadelta(Et_1, g, gamma, RMS_1, e):\n",
    "    Et = (gamma*Et_1) + (1-gamma)*(g**2)\n",
    "    RMS = np.sqrt(Et + e)\n",
    "    delta = (RMS_1/RMS)*g\n",
    "    return Et, RMS, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(thetas, X, dropout_prob, train_test_check):\n",
    "    num_layers = thetas.shape[0]\n",
    "    outs = []\n",
    "    for t in range(0, num_layers): #проходим по всем слоям (все слои с сигмоидальной функцией активации)\n",
    "        if t == 0:\n",
    "            outs.append(np.array([h(thetas[t][:, i], X) for i in range(0, thetas[t].shape[1])]).T)\n",
    "        else:\n",
    "            outs.append(np.array([h(thetas[t][:, i], np.asarray(outs[t-1])) for i in range(0, thetas[t].shape[1])]).T)\n",
    "    #Dropout\n",
    "    if train_test_check:\n",
    "        for i in range(len(outs) - 1):#для всех слоев кроме выходного и входного\n",
    "            for o in outs[i]:\n",
    "                o *= np.random.binomial(1,dropout_prob,o.shape[0]) * (1) / (dropout_prob)\n",
    "    return [out for out in outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y_gt, thetas, dropout_prob):\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = y_gt.shape[1]\n",
    "    outs = forward(thetas, X,dropout_prob, True)\n",
    "    num_layers = len(outs)\n",
    "    d_theta = []\n",
    "    d_theta.append(np.zeros([num_features, outs[0].shape[1]]))\n",
    "    for i in range(0, num_layers - 1):\n",
    "        d_theta.append(np.zeros([outs[i].shape[1], outs[i+1].shape[1]]))\n",
    "    tmp_th = None\n",
    "    #скрытые слои\n",
    "    for i in range(len(outs) - 1, -1, -1):\n",
    "        if i == len(outs) - 1:#выходной слой\n",
    "            for class_no in range(0, num_classes):\n",
    "                d_theta[i][:, class_no] = CEderivative(outs[i-1], y_gt[:, class_no], outs[i][:, class_no])\n",
    "            tmp_th = outs[i] - y_gt\n",
    "        elif i == 0:#первый скрытый\n",
    "            tmp_th = np.dot(tmp_th, thetas[1].T)\n",
    "            tmp_th = tmp_th * sigm_derivative(thetas[0], X)\n",
    "            d_theta[0] = np.dot(X.T, tmp_th)\n",
    "            d_theta[0] = d_theta[0]/outs[0].shape[0]\n",
    "        else:# остальные скрытые\n",
    "            tmp_th = np.dot(tmp_th, thetas[i+1].T)\n",
    "            tmp_th = tmp_th * sigm_derivative(thetas[i], outs[i-1])\n",
    "            d_theta[i] = np.dot(outs[i-1].T, tmp_th)\n",
    "            d_theta[i] = d_theta[i]/outs[i].shape[0]\n",
    "    return d_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dx_train, dy_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', dx_train.shape)\n",
    "print(dx_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_train = encode_lable(dy_train, 10)\n",
    "y_test = encode_lable(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_train = np.asarray([list(x.flat) for x in dx_train])\n",
    "x_test = np.asarray([list(x.flat) for x in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (40000, 3072)\n",
      "y_train shape: (40000, 10)\n",
      "x_val shape: (10000, 3072)\n",
      "y_val shape: (10000, 10)\n",
      "x_test shape: (10000, 3072)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "p = np.random.permutation(dx_train.shape[0])\n",
    "x_train = dx_train[p[0:int(len(dx_train)*0.8)], :]\n",
    "y_train = dy_train[p[0:int(len(dx_train)*0.8)]]\n",
    "x_val = dx_train[p[int(len(dx_train)*0.8):], :]\n",
    "y_val = dy_train[p[int(len(dx_train)*0.8):]]\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - np.mean(x_train, axis = 0)) / np.var(x_train, axis = 0)\n",
    "x_val = (x_val - np.mean(x_val, axis = 0)) / np.var(x_val, axis = 0)\n",
    "x_test = (x_test - np.mean(x_test, axis = 0)) / np.var(x_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_thetas(layers_size):\n",
    "    np.random.seed(0)\n",
    "    list_thetas = []\n",
    "    for i in range(0, len(layers_size)-2):\n",
    "        list_thetas.append(np.random.uniform(-1*pow(layers_size[i+1], (1/float(layers_size[i]))), pow(layers_size[i+1], (1/float(layers_size[i]))), (layers_size[i], layers_size[i+1])))\n",
    "    list_thetas.append(np.random.uniform(-0.5, 0.5,(layers_size[-2], layers_size[-1])))\n",
    "    return np.asarray(list_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_init_thetas(layers_size):\n",
    "    list_thetas = []\n",
    "    for i in range(0, len(layers_size) - 1):\n",
    "        r = (4*(6/(layers_size[i] + layers_size[i+1]))**1/2)\n",
    "        list_thetas.append(np.random.uniform(-1*r, r, (layers_size[i], layers_size[i+1])))\n",
    "    return np.asarray(list_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss : 7.673839930561796\n",
      "\n",
      "\n",
      "Epoch 0/50 .."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-609461e489b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mEt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMSprob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_theta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Adadelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;31m#Et[j], delta = RMSprob(Et[j], d_theta[j], 0.9, alpha, 10**-8) #RMSprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m#thetas[j] =thetas[j] - alpha*d_theta[j] #Обычный mini-batch BackProp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "dropout_prob = 0.7\n",
    "num_epochs = 50\n",
    "alpha = 0.01\n",
    "history_loss_train,history_loss_val, history_acc_train, history_acc_val = [],[],[],[]\n",
    "num_train_samples = x_train.shape[0]\n",
    "num_batches = num_train_samples // batch_size\n",
    "layers_size = [3072, 64 ,10]\n",
    "#thetas = np.asarray([np.random.randn(layers_size[i], layers_size[i+1]) for i in range(0, len(layers_size)-1)])\n",
    "thetas = init_thetas(layers_size)\n",
    "y_pred = forward(thetas, x_train,dropout_prob, False)\n",
    "loss_val = loss(y_train, y_pred[-1])\n",
    "print(\"Initial loss :\", loss_val)\n",
    "Et = np.asarray([np.zeros(th.shape) for th in thetas])\n",
    "RMS = np.asarray([np.zeros(th.shape) for th in thetas])\n",
    "for i in range(0, num_epochs):\n",
    "    p = np.random.permutation(num_train_samples)\n",
    "    print('\\n\\nEpoch %d/%d ' % (i, num_epochs), end='.')\n",
    "    for batch_no in range(0, num_batches):\n",
    "        if np.mod(batch_no, num_batches // 10) == 0:\n",
    "            print('.', end='')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        x_train_batch = x_train[p[batch_no * batch_size : (batch_no + 1) * batch_size], :]\n",
    "        y_train_batch = y_train[p[batch_no * batch_size: (batch_no + 1) * batch_size]]\n",
    "        d_theta = backward(x_train_batch, y_train_batch, thetas,dropout_prob)\n",
    "        \n",
    "        for j in range(0,len(d_theta)):\n",
    "            Et[j], RMS[j], delta = RMSprob(Et[j], d_theta[j], 0.9, RMS[j], 10**-8) #Adadelta\n",
    "            #Et[j], delta = RMSprob(Et[j], d_theta[j], 0.9, alpha, 10**-8) #RMSprop\n",
    "            #thetas[j] =thetas[j] - alpha*d_theta[j] #Обычный mini-batch BackProp\n",
    "            thetas[j] = thetas[j] - delta \n",
    "  \n",
    "    #loss train\n",
    "    pred_train_probs = forward(thetas, x_train,dropout_prob, False)\n",
    "    y_train_out = pred_train_probs[-1]\n",
    "    loss_train = loss(y_train,  y_train_out)\n",
    "    history_loss_train.append(loss_train)\n",
    "    \n",
    "    #loss val\n",
    "    pred_val_probs = forward(thetas, x_val,dropout_prob, False)\n",
    "    y_val_out = pred_val_probs[-1]\n",
    "    loss_val = loss(y_val, y_val_out)\n",
    "    history_loss_val.append(loss_val)\n",
    "    \n",
    "    history_acc_train.append(acc(y_train, y_train_out))\n",
    "    history_acc_val.append(acc(y_val, y_val_out))\n",
    "    print(\"\\nLoss train: \", loss_train,\n",
    "          \"\\nLoss val: \", loss_val,\n",
    "          \"\\nTrain accuracy: \",acc(y_train, y_train_out),\n",
    "          \"\\nValidation accuracy: \", acc(y_val, y_val_out))\n",
    "    print(\"Alpha: \", alpha)\n",
    "    #alpha = max(0.00001, alpha * 0.5)#изменение шага градиента (сначала идем большими, постепенно уменьшая)\n",
    "    if len(history_loss_val) >= 2:\n",
    "        if history_loss_val[-2] - history_loss_val[-1] < 0.0005:\n",
    "            alpha = max(alpha/2, 0.001)\n",
    "    if (np.mod(i, num_epochs // (num_epochs/5)) == 0) and i != 0:#каждые 10 выводим на экран\n",
    "        plt.plot(history_loss_train)\n",
    "        plt.plot(history_loss_val)\n",
    "        plt.title(f'epoch {i}: model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history_acc_train)\n",
    "        plt.plot(history_acc_val)\n",
    "        plt.title(f'epoch {i}: model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_loss_train)\n",
    "plt.plot(history_loss_val)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_acc_train)\n",
    "plt.plot(history_acc_val)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_probs = forward(thetas, x_test,dropout_prob, False)\n",
    "y_test_out = pred_test_probs[-1]\n",
    "loss_test = loss(y_test, y_test_out)\n",
    "print(\"\\nLoss test:\", loss_test,\n",
    "          \"\\nTest accuracy : \",acc(y_test, y_test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(thetas, open('weights\\\\thetasTwoHidLay64-348_RSMprob_50epochs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('results_Khayrkina5710M.txt','w')\n",
    "for y in y_test_out:\n",
    "    f.write(str(np.argmax(y))+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
